{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSML1010 Group3 Course_Project - Milestone 2 - Baseline Machine Learning Implementation\n",
    "#### Authors (Group3): Paul Doucet, Jerry Khidaroo\n",
    "#### Project Repository: https://github.com/CSML1010-3-2020/NLPCourseProject\n",
    "\n",
    "#### Dataset:\n",
    "The dataset used in this project is the __Taskmaster-1__ dataset from Google.\n",
    "[Taskmaster-1](https://research.google/tools/datasets/taskmaster-1/)\n",
    "\n",
    "The dataset can be obtained from: https://github.com/google-research-datasets/Taskmaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workbook Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas, numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Some Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust pandas display\n",
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.float_format = '{:.7f}'.format\n",
    "pd.options.display.precision = 7\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "# Import matplotlib and seaborn and adjust some defaults\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Instruction_id', 'category', 'selfdialog_norm'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('./data/dialog_norm.csv')\n",
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instruction_id</th>\n",
       "      <th>category</th>\n",
       "      <th>selfdialog_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>restaurant-table-2</td>\n",
       "      <td>0</td>\n",
       "      <td>hi im looking book table korean fod ok area thinking somewhere southern nyc maybe east village ok great theres thursday kitchen great reviews thats great need table tonight pm people dont want sit bar anywhere else fine dont availability pm times available yikes cant times ok second choice let check ok lets try boka free people yes great lets book ok great requests thats book great use account open yes please great get confirmation phone soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie-tickets-1</td>\n",
       "      <td>1</td>\n",
       "      <td>hi would like see movie men want playing yes showing would like purchase ticket yes friend two tickets please okay time moving playing today movie showing pm okay anymore movies showing around pm yes showing pm green book two men dealing racisim oh recommend anything else like well like movies funny like comedies well like action well okay train dragon playing pm okay get two tickets want cancel tickets men want yes please okay problem much cost said two adult tickets yes okay okay anything else help yes bring food theater sorry purchase food lobby okay fine thank enjoy movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie-tickets-3</td>\n",
       "      <td>2</td>\n",
       "      <td>want watch avengers endgame want watch bangkok close hotel currently staying sounds good time want watch movie oclock many tickets two use account already movie theater yes seems movie time lets watch another movie movie want watch lets watch train dragon newest one yes one dont think movie playing time either neither choices playing time want watch afraid longer interested watching movie well great day sir thank welcome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Instruction_id  category  \\\n",
       "0  restaurant-table-2         0   \n",
       "1     movie-tickets-1         1   \n",
       "2     movie-tickets-3         2   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          selfdialog_norm  \n",
       "0                                                                                                                                          hi im looking book table korean fod ok area thinking somewhere southern nyc maybe east village ok great theres thursday kitchen great reviews thats great need table tonight pm people dont want sit bar anywhere else fine dont availability pm times available yikes cant times ok second choice let check ok lets try boka free people yes great lets book ok great requests thats book great use account open yes please great get confirmation phone soon  \n",
       "1  hi would like see movie men want playing yes showing would like purchase ticket yes friend two tickets please okay time moving playing today movie showing pm okay anymore movies showing around pm yes showing pm green book two men dealing racisim oh recommend anything else like well like movies funny like comedies well like action well okay train dragon playing pm okay get two tickets want cancel tickets men want yes please okay problem much cost said two adult tickets yes okay okay anything else help yes bring food theater sorry purchase food lobby okay fine thank enjoy movie  \n",
       "2                                                                                                                                                                want watch avengers endgame want watch bangkok close hotel currently staying sounds good time want watch movie oclock many tickets two use account already movie theater yes seems movie time lets watch another movie movie want watch lets watch train dragon newest one yes one dont think movie playing time either neither choices playing time want watch afraid longer interested watching movie well great day sir thank welcome  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove NaN rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7708, 3)\n",
      "(7705, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_all.shape)\n",
    "df_all = df_all.dropna()\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a Sample of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size:  3000 sample_per_cat:  214\n"
     ]
    }
   ],
   "source": [
    "cat_id_df = df_all[['Instruction_id', 'category']].drop_duplicates().sort_values('category')\n",
    "cat_count = len(cat_id_df)\n",
    "sample_size = 3000\n",
    "sample_per_cat = sample_size//cat_count\n",
    "print('sample_size: ', sample_size, 'sample_per_cat: ', sample_per_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction_id\n",
      "auto-repair-appt-1    248\n",
      "coffee-ordering-1     243\n",
      "coffee-ordering-2     243\n",
      "movie-finder           50\n",
      "movie-tickets-1       239\n",
      "movie-tickets-2       248\n",
      "movie-tickets-3       187\n",
      "pizza-ordering-1      238\n",
      "pizza-ordering-2      241\n",
      "restaurant-table-1    242\n",
      "restaurant-table-2    241\n",
      "restaurant-table-3     94\n",
      "uber-lyft-1           249\n",
      "uber-lyft-2           237\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to Get balanced Sample - Get a bit more than needed then down sample\n",
    "def sampling_k_elements(group, k=sample_per_cat + 40):\n",
    "    if len(group) < k:\n",
    "        return group\n",
    "    return group.sample(k, random_state=5)\n",
    "\n",
    "#Get balanced samples\n",
    "corpus_df = df_all.groupby('Instruction_id').apply(sampling_k_elements).reset_index(drop=True)\n",
    "\n",
    "#Reduce to sample_size\n",
    "corpus_df = corpus_df.sample(n=sample_size, random_state=3)\n",
    "print (corpus_df.groupby('Instruction_id').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Corpus List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hey need reserve movie tickets glass sure would like purchase louisville preston crossings day wanting go tonight pm work many tickets need ok give second much tickets ok works confirm want tickets tonights showing glass preston crossings thats right want send tickets phone text message yes would perfect ok tickets way got text thank much youre welcome',\n",
       " 'hey schedule appointment repair car yes prefer certain shop called intelligent auto solutions one location alright pulled need make model year car jeep wrangler seems issue might imagination feel like pulling right recently sometimes drive straight highway take hands steering wheel end realign within lane within seconds theyll want bring morning urgent want look willing dont open spots see today appointments day closing okay ill bring morning tomorrow dont open oh work go work type job take hours would fixed time drive work anyway itll wait work oclock open book fantastic thanks asking name good contact number reach finalize appointment make maisel confirm appointment set repair wheel alignment jeep wrangler pm tomorrow much cost inspection',\n",
       " 'need restaurant reservation tonight sure kind restaurant interested im entirely sure feel like fish maybe sushi seafood restaurant ok want eat cleveland yes please cleveland ok well two highest rated places alley cat oyster bar parallax sushi oh heard never either one better yelp rating close parallax sushi restaurant rated slightly higher ok parallax bar yes full service bar oh good need drink make reservation pm tonight party two reserve table bar ok opening tonight pm take reservations bar grrr ok well take table dining room hopefully get open table bar sit ok sounds good done ok going hate kind want oysters could never hate best boss also good news parallax also oysters get want keep reservation change oh thats great news think keep ok sounds great anything else yes said oysters mean menu online send email anything else nope cant wait look menu absolutely enjoy dinner',\n",
       " 'want reserve table per se okay date looking saturday april time pm many party people per se table availability april next availabilty may tables april good check cookshop table cookshop available april party time would like pm pm available pm pm dont work need reservation pm times available ok dont want reserve anything today thanks help welcome bye goodbye']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lst = []\n",
    "for i, row in corpus_df.iterrows():\n",
    "    doc_lst.append(row.selfdialog_norm)\n",
    "\n",
    "print(len(doc_lst))\n",
    "doc_lst[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_lst, corpus_df['category'], test_size=0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9051\n",
      "Vocabulary Sample: [('like', 1), ('would', 2), ('ok', 3), ('okay', 4), ('yes', 5), ('want', 6), ('pm', 7), ('order', 8), ('thank', 9), ('please', 10)]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in X_train]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 8, 3, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 4, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 6, 6, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1., vocabulary=word2id)\n",
    "cv_matrix = cv.fit_transform(X_train, y_train)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAD</th>\n",
       "      <th>like</th>\n",
       "      <th>would</th>\n",
       "      <th>ok</th>\n",
       "      <th>okay</th>\n",
       "      <th>yes</th>\n",
       "      <th>want</th>\n",
       "      <th>pm</th>\n",
       "      <th>order</th>\n",
       "      <th>thank</th>\n",
       "      <th>please</th>\n",
       "      <th>tickets</th>\n",
       "      <th>one</th>\n",
       "      <th>time</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>marinare</th>\n",
       "      <th>reviewing</th>\n",
       "      <th>lawrence</th>\n",
       "      <th>ria</th>\n",
       "      <th>myles</th>\n",
       "      <th>hawthorn</th>\n",
       "      <th>amend</th>\n",
       "      <th>wlll</th>\n",
       "      <th>allie</th>\n",
       "      <th>extea</th>\n",
       "      <th>temporarialy</th>\n",
       "      <th>cannery</th>\n",
       "      <th>iwilei</th>\n",
       "      <th>scattered</th>\n",
       "      <th>wowdo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2250 rows × 9051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PAD  like  would  ok  okay  yes  want  pm  order  thank  please  \\\n",
       "0       0     8      3  10     0    2     4   7      0      1       2   \n",
       "1       0     1      1   9     0    2     1   2      0      0       0   \n",
       "2       0     4      0   0     0    3     2   0      4      1       0   \n",
       "3       0     0      0   0     3    1     0   0      1      0       0   \n",
       "4       0     0      0   0     4    1     2   0      0      1       0   \n",
       "...   ...   ...    ...  ..   ...  ...   ...  ..    ...    ...     ...   \n",
       "2245    0     1      1   0     4    0     1   1      0      1       1   \n",
       "2246    0     2      1   0     3    1     0   6      0      1       0   \n",
       "2247    0     0      0   0     4    2     1   0      0      0       0   \n",
       "2248    0     6      6   1     2    1     2   0      7      0       0   \n",
       "2249    0     0      0   0     0    0     0   0      0      1       2   \n",
       "\n",
       "      tickets  one  time  great  ...  marinare  reviewing  lawrence  ria  \\\n",
       "0           0    0     4      2  ...         0          0         0    0   \n",
       "1           0    1     0      0  ...         0          0         0    0   \n",
       "2           0    5     1      0  ...         0          0         0    0   \n",
       "3           0    6     0      2  ...         0          0         0    0   \n",
       "4           4    0     0      0  ...         0          0         0    0   \n",
       "...       ...  ...   ...    ...  ...       ...        ...       ...  ...   \n",
       "2245        4    0     2      1  ...         0          0         0    0   \n",
       "2246        0    0     1      0  ...         0          0         0    0   \n",
       "2247        2    0     1      1  ...         0          0         0    0   \n",
       "2248        0    1     0      0  ...         0          0         0    0   \n",
       "2249        0    0     1      0  ...         0          0         0    0   \n",
       "\n",
       "      myles  hawthorn  amend  wlll  allie  extea  temporarialy  cannery  \\\n",
       "0         0         0      0     0      0      0             0        0   \n",
       "1         0         0      0     0      0      0             0        0   \n",
       "2         0         0      0     0      0      0             0        0   \n",
       "3         0         0      0     0      0      0             0        0   \n",
       "4         0         0      0     0      0      0             0        0   \n",
       "...     ...       ...    ...   ...    ...    ...           ...      ...   \n",
       "2245      0         0      0     0      0      0             0        0   \n",
       "2246      0         0      0     0      0      0             0        0   \n",
       "2247      0         0      0     0      0      0             0        1   \n",
       "2248      0         0      0     0      0      0             0        0   \n",
       "2249      0         0      0     0      0      0             0        0   \n",
       "\n",
       "      iwilei  scattered  wowdo  \n",
       "0          0          0      0  \n",
       "1          0          0      0  \n",
       "2          0          0      0  \n",
       "3          0          0      0  \n",
       "4          0          0      0  \n",
       "...      ...        ...    ...  \n",
       "2245       0          0      0  \n",
       "2246       0          0      0  \n",
       "2247       1          1      1  \n",
       "2248       0          0      0  \n",
       "2249       0          0      0  \n",
       "\n",
       "[2250 rows x 9051 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 9051)\n",
      "(750, 9051)\n",
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "# Get BOW features\n",
    "X_train_bow = cv_matrix #cv.fit_transform(X_train).toarray()\n",
    "X_test_bow = cv.transform(X_test).toarray()\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print (X_train_bow.shape) \n",
    "print (X_test_bow.shape) \n",
    "print (y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Builder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "class Result_Metrics:\n",
    "    def __init__(self, predicter, cm, report, f1_score, accuracy, precision, recall):\n",
    "        self.predicter = predicter\n",
    "        self.cm = cm    # instance variable unique to each instance\n",
    "        self.report = report\n",
    "        self.f1_score = f1_score\n",
    "        self.accuracy = accuracy\n",
    "        self.precision = precision\n",
    "        self.recall = recall\n",
    "\n",
    "def Build_Model(model, features_train, labels_train, features_test, labels_test):\n",
    "    classifier = model.fit(features_train, labels_train)\n",
    "\n",
    "    # Predicter to output\n",
    "    pred = classifier.predict(features_test)\n",
    "\n",
    "    # Metrics to output\n",
    "    cm = confusion_matrix(pred,labels_test)\n",
    "    report = metrics.classification_report(labels_test, pred)\n",
    "    f1 = metrics.f1_score(labels_test, pred, average='weighted')\n",
    "    accuracy = cm.trace()/cm.sum()\n",
    "    precision = metrics.precision_score(labels_test, pred, average='weighted')\n",
    "    recall = metrics.recall_score(labels_test, pred, average='weighted')\n",
    "    \n",
    "    rm = Result_Metrics(pred, cm, report, f1, accuracy, precision, recall)\n",
    "\n",
    "    return rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Feature Benchmarking Baseline with Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_nb_bow = MultinomialNB()\n",
    "rm_nb_bow = Build_Model(model_nb_bow, X_train_bow, y_train, X_test_bow, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Save_Benchmark(descr, feat_type, b_metrics, reset_rb, reset_rb_all):\n",
    "    global rows_benchmarks\n",
    "    global rows_benchmarks_all\n",
    "    global df_benchmarks\n",
    "    global df_benchmarks_all\n",
    "    if (reset_rb):\n",
    "        rows_benchmarks = []\n",
    "\n",
    "    if (reset_rb_all):\n",
    "        rows_benchmarks_all = []\n",
    "    rows_benchmarks.append([descr, feat_type, b_metrics.precision, b_metrics.recall, b_metrics.f1_score, b_metrics.accuracy])\n",
    "    rows_benchmarks_all.append([descr, feat_type, b_metrics.precision, b_metrics.recall, b_metrics.f1_score, b_metrics.accuracy])\n",
    "    df_benchmarks = pd.DataFrame(rows_benchmarks, columns=[\"Features_Benchedmarked\", \"Feat_Type\", \"Precision\", \"Recall\", \"f1_score\", \"accuracy\"])\n",
    "    df_benchmarks_all = pd.DataFrame(rows_benchmarks_all, columns=[\"Features_Benchedmarked\", \"Feat_Type\", \"Precision\", \"Recall\", \"f1_score\", \"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark output\n",
    "Save_Benchmark(\"BOW Naive Bayes Baseline\", \"BOW\", rm_nb_bow, True, True)\n",
    "#df_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#rm_nb_bow.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#print(\"Label\" + rm_nb_bow.report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection: BOW Features with Naive Bayes Model Using Chi-Squared Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Feature Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "class Result_Metrics_selected:\n",
    "    def __init__(self, x_train_sel, x_test_sel, predicter, cm, report, f1_score, accuracy, precision, recall):\n",
    "        self.x_train_sel = x_train_sel\n",
    "        self.x_test_sel = x_test_sel\n",
    "        self.predicter = predicter\n",
    "        self.cm = cm    # instance variable unique to each instance\n",
    "        self.report = report\n",
    "        self.f1_score = f1_score\n",
    "        self.accuracy = accuracy\n",
    "        self.precision = precision\n",
    "        self.recall = recall\n",
    "\n",
    "def Get_Scaled_Features(features_train, labels_train, features_test, labels_test, scaler):\n",
    "    x_train_scaled = scaler.fit_transform(features_train, labels_train)\n",
    "    x_test_scaled = scaler.transform(features_test)\n",
    "    return x_train_scaled, x_test_scaled\n",
    "\n",
    "def Select_Best_Features_Chi(num_feats, features_train, labels_train, features_test, labels_test):\n",
    "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
    "    chi_selector.fit(features_train, labels_train)\n",
    "    chi_support = chi_selector.get_support()    \n",
    "    X_train_chi = features_train[:,chi_support]\n",
    "    X_test_chi = features_test[:,chi_support]\n",
    "    return X_train_chi, X_test_chi\n",
    "\n",
    "def Get_Model_Feature_Metrics(model, num_feats, features_train, labels_train, features_test, labels_test, scaler):\n",
    "    X_train_chi, X_test_chi = Select_Best_Features_Chi(num_feats, features_train, labels_train, features_test, labels_test)\n",
    "    x_train_scaled, x_test_scaled = Get_Scaled_Features(X_train_chi, labels_train, X_test_chi, labels_test, scaler)\n",
    "    rm_chi = Build_Model(model, x_train_scaled, labels_train, x_test_scaled, labels_test)\n",
    "    return rm_chi\n",
    "\n",
    "def SelectBestModelFeatures_Chi(model, num_feats, features_train, labels_train, features_test, labels_test, scaler):\n",
    "    X_norm = scaler.fit_transform(features_train, labels_train)\n",
    "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
    "    chi_selector.fit(X_norm, labels_train)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    \n",
    "    X_train_chi = features_train[:,chi_support]\n",
    "    X_test_chi = features_test[:,chi_support]\n",
    "\n",
    "    classifier_chi = model.fit(X_train_chi, labels_train)\n",
    "\n",
    "    # Predicter to output\n",
    "    predict_chi = classifier_chi.predict(X_test_chi)\n",
    "\n",
    "    # Metrics to output\n",
    "    cm_chi = confusion_matrix(predict_chi,labels_test)\n",
    "    report_chi = metrics.classification_report(labels_test, predict_chi)\n",
    "    f1_chi = metrics.f1_score(labels_test, predict_chi, average='weighted')\n",
    "    accuracy_chi = cm_chi.trace()/cm_chi.sum()\n",
    "    precision_chi = metrics.precision_score(labels_test, predict_chi, average='weighted')\n",
    "    recall_chi = metrics.recall_score(labels_test, predict_chi, average='weighted')\n",
    "    \n",
    "    rm_chi = Result_Metrics_selected(X_train_chi, X_test_chi, predict_chi, cm_chi, report_chi, f1_chi, accuracy_chi, precision_chi, recall_chi)\n",
    "\n",
    "    return rm_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate through number of features and get benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 9000 100\n"
     ]
    }
   ],
   "source": [
    "a = 100\n",
    "tot = X_train_bow.shape[1]\n",
    "b = 100 * (tot//100)\n",
    "c = 100\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "scaler_min_max = MinMaxScaler()\n",
    "for i in range(a, b, c): # range(a, b, c) will count from a to b by intervals of c.\n",
    "    #rm_chi_i = Get_Model_Feature_Metrics(model_nb_bow, i, X_train_bow, y_train, X_test_bow, y_test, scaler_min_max)\n",
    "    rm_chi_i = SelectBestModelFeatures_Chi(model_nb_bow, i, X_train_bow, y_train, X_test_bow, y_test, scaler_min_max)\n",
    "    rows.append([i, rm_chi_i.f1_score, rm_chi_i.accuracy])\n",
    "\n",
    "acc_df = pd.DataFrame(rows, columns=[\"num_of_features\", \"f1_score\", \"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot f1-score by number of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_df.plot(x=\"num_of_features\", y=\"f1_score\", title=\"F1 Score by Number of Selected Features - BOW with Naive Bayes\", figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 850 1\n"
     ]
    }
   ],
   "source": [
    "Opt_no_of_feat = int(acc_df.sort_values(by='f1_score', ascending=False).iloc[0]['num_of_features'])\n",
    "Opt_no_of_feat\n",
    "a = Opt_no_of_feat - 50\n",
    "b = Opt_no_of_feat + 50\n",
    "c = 1\n",
    "print(a, b, c)\n",
    "#acc_df.sort_values(by='f1_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a more fine-grained look at the optimal number of features region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(a, b, c): # range(a, b, c) will count from a to b by intervals of c.\n",
    "    #rm_chi_i = Get_Model_Feature_Metrics(model_nb_bow, i, X_train_bow, y_train, X_test_bow, y_test, scaler_min_max)\n",
    "    rm_chi_i = SelectBestModelFeatures_Chi(model_nb_bow, i, X_train_bow, y_train, X_test_bow, y_test, scaler_min_max)\n",
    "    rows.append([i, rm_chi_i.f1_score, rm_chi_i.accuracy])\n",
    "\n",
    "acc_df = pd.DataFrame(rows, columns=[\"num_of_features\", \"f1_score\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_df.plot(x=\"num_of_features\", y=\"f1_score\", title=\"F1 Score by Number of Selected Features - BOW with Naive Bayes\", figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794\n"
     ]
    }
   ],
   "source": [
    "Opt_no_of_feat = int(acc_df.sort_values(by='f1_score', ascending=False).iloc[0]['num_of_features'])\n",
    "print(Opt_no_of_feat)\n",
    "#acc_df.sort_values(by='f1_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark BOW With Optimal Features Selected using Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb_bow_opt = MultinomialNB()\n",
    "rm_chi_opt_bow = SelectBestModelFeatures_Chi(model_nb_bow, Opt_no_of_feat, X_train_bow, y_train, X_test_bow, y_test, scaler_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rm_chi_opt_bow.cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Label\" + rm_chi_opt_bow.report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features_Benchedmarked</th>\n",
       "      <th>Feat_Type</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOW Naive Bayes Baseline</td>\n",
       "      <td>BOW</td>\n",
       "      <td>0.6990870</td>\n",
       "      <td>0.6946667</td>\n",
       "      <td>0.6843059</td>\n",
       "      <td>0.6946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOW Naive Bayes Optimal Features Selected: 794</td>\n",
       "      <td>BOW</td>\n",
       "      <td>0.7698257</td>\n",
       "      <td>0.7520000</td>\n",
       "      <td>0.7431145</td>\n",
       "      <td>0.7520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Features_Benchedmarked Feat_Type  Precision  \\\n",
       "0                        BOW Naive Bayes Baseline       BOW  0.6990870   \n",
       "1  BOW Naive Bayes Optimal Features Selected: 794       BOW  0.7698257   \n",
       "\n",
       "     Recall  f1_score  accuracy  \n",
       "0 0.6946667 0.6843059 0.6946667  \n",
       "1 0.7520000 0.7431145 0.7520000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save benchmark output\n",
    "Save_Benchmark(\"BOW Naive Bayes Optimal Features Selected: \" + str(Opt_no_of_feat), \"BOW\", rm_chi_opt_bow, False, False)\n",
    "df_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Benchmark Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the following four models: Logistic Regression (Multinomial) Naive Bayes Linear Support Vector Machine Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model_ids = ['RF', 'SVC', 'NB','LR']\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0, max_iter=500),\n",
    "]\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "\n",
    "for model, model_id in zip(models, model_ids):\n",
    "    model_name = model.__class__.__name__\n",
    "    f1_scores = cross_val_score(model, X_train_bow, y_train, scoring='f1_weighted', cv=CV)\n",
    "    #precisions = cross_val_score(model, X_train_bow, y_train, scoring='precision_weighted', cv=CV)\n",
    "    #recalls = cross_val_score(model, X_train_bow, y_train, scoring='recall_weighted', cv=CV)\n",
    "    \n",
    "    for i in range(0, 9, 1):\n",
    "        entries.append((model_id, model_name, 'baseline', 'default', '', f1_scores[i]))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['Model_Id', 'Model', 'Features', 'Hyper_Param', 'Best_Params', 'F1_Score'])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimised Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_jobs=-1),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(n_jobs=-1)\n",
    "]\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "#entries = []\n",
    "\n",
    "for model, model_id in zip(models, model_ids):\n",
    "    model_name = model.__class__.__name__\n",
    "    f1_scores = cross_val_score(model, rm_chi_opt_bow.x_train_sel, y_train, scoring='f1_weighted', cv=CV)\n",
    "    #precisions = cross_val_score(model, rm_chi_opt_bow.x_train_sel, y_train, scoring='precision_weighted', cv=CV)\n",
    "    #recalls = cross_val_score(model, rm_chi_opt_bow.x_train_sel, y_train, scoring='recall_weighted', cv=CV)\n",
    "\n",
    "    for i in range(0, 9, 1):\n",
    "        entries.append((model_id, model_name, 'optimized', 'default', '', f1_scores[i]))\n",
    "        #entries.append((model_name, 'optimized', precisions[i], recalls[i], f1_scores[i]))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['Model_Id','Model', 'Features', 'Hyper_Param', 'Best_Params', 'F1_Score'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = cv_df.groupby(['Model_Id', 'Model','Features', 'Hyper_Param', 'Best_Params']).agg(['mean'])\n",
    "models_df.columns = models_df.columns.map('_'.join)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, sharex=True)\n",
    "sns.boxplot(x='Model', y='F1_Score', data=cv_df, hue='Features', ax=ax1);\n",
    "sns.stripplot(x='Model', y='F1_Score', data=cv_df, hue='Features', size=6, jitter=True, edgecolor=\"gray\", linewidth=2, ax=ax1);\n",
    "sns.barplot(y='F1_Score', x='Model', data=cv_df, palette=\"colorblind\", hue='Features', ax=ax2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the Hyperparameters Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class Estimator_Parameters:\n",
    "    def __init__(self, estimator, parameters, feat_type, x, y):\n",
    "        self.estimator = estimator\n",
    "        self.parameters = parameters\n",
    "        self.feat_type = feat_type\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "def Get_Best_Parameters(est_param):\n",
    "    grid_search = GridSearchCV(estimator = est_param.estimator,\n",
    "                            param_grid = est_param.parameters,\n",
    "                            scoring = 'f1_weighted',\n",
    "                            cv= 10,\n",
    "                            n_jobs = -1)\n",
    "    grid_search = grid_search.fit(est_param.x, est_param.y)\n",
    "    return grid_search.best_score_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "est_param_arr = [\n",
    "    Estimator_Parameters(RandomForestClassifier(), [{'n_estimators': [50,100,150,200,250,300],'max_depth': [1, 2, 3,4,5], 'random_state': [0,1,2,3]}], \"optimized\", rm_chi_opt_bow.x_train_sel, y_train),\n",
    "    Estimator_Parameters(LinearSVC(), [{'C': [1000, 1400, 1500, 1600],'loss': ['hinge', 'squared_hinge'], 'dual': [True, False], 'penalty': ['l1', 'l2'], 'max_iter': [1000, 1500, 2000]}], \"optimized\", rm_chi_opt_bow.x_train_sel, y_train),\n",
    "    Estimator_Parameters(MultinomialNB(), [{'alpha': [0.1,0.2,0.3,0.4,0.42,0.44,0.46,0.48,0.5],'fit_prior': [True, False]}], \"optimized\", rm_chi_opt_bow.x_train_sel, y_train),\n",
    "    Estimator_Parameters(LogisticRegression(), [{'C': [1,2,3], 'penalty': ['l1', 'l2', 'elasticnet', 'none'],'dual': [True, False], 'multi_class': ['auto', 'ovr', 'multinomial']}], \"optimized\", rm_chi_opt_bow.x_train_sel, y_train)\n",
    "]\n",
    "\n",
    "for est_param, model_id in zip(est_param_arr, model_ids):\n",
    "    estimator_name = est_param.estimator.__class__.__name__\n",
    "    best_accuracy, best_parameters = Get_Best_Parameters(est_param)\n",
    "    entries.append([model_id, estimator_name, est_param.feat_type, 'tuned', str(best_parameters), best_accuracy])\n",
    "    print(estimator_name, best_accuracy, best_parameters, est_param.feat_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(entries, columns=['Model_Id', 'Model', 'Features', 'Hyper_Param', 'Best_Params', 'F1_Score'])\n",
    "models_df = result_df.groupby(['Model_Id', 'Model','Features','Hyper_Param', 'Best_Params']).agg(['mean'])\n",
    "models_df.columns = models_df.columns.map('_'.join)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. a. Learning Curves: Training/ Testing Errors - Optimized Hyperarameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_learning_curves\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=300, max_depth=5, random_state=1),\n",
    "    LinearSVC(C=1400, dual=False, loss='squared_hinge', max_iter=1500, penalty='l1'),\n",
    "    MultinomialNB(alpha=0.2, fit_prior=True),\n",
    "    LogisticRegression(C=1, dual=False, multi_class='ovr', penalty='l2'),\n",
    "]\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for model, grd in zip(models, grid):    \n",
    "    model_name = model.__class__.__name__\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig2 = plot_learning_curves(rm_chi_opt_bow.x_train_sel, y_train, rm_chi_opt_bow.x_test_sel, y_test, model, print_model=False, style='ggplot')\n",
    "    plt.title(model_name)\n",
    "         \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. b. Learning Curves: Training/Testing Accuracy - Optimized Hyperarameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_learning_curves\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=300, max_depth=5, random_state=1),\n",
    "    LinearSVC(C=1400, dual=False, loss='squared_hinge', max_iter=1500, penalty='l1'),\n",
    "    MultinomialNB(alpha=0.2, fit_prior=True),\n",
    "    LogisticRegression(C=1, dual=False, multi_class='ovr', penalty='l2'),\n",
    "]\n",
    "\n",
    "#fig2 = plt.figure(figsize=(10, 10))\n",
    "fig3 = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for model, grd in zip(models, grid):    \n",
    "    model_name = model.__class__.__name__\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    #fig2 = plot_learning_curves(rm_chi_opt_bow.x_train_sel, y_train, rm_chi_opt_bow.x_test_sel, y_test, model, print_model=False, style='ggplot')\n",
    "    fig3 = plot_learning_curves(rm_chi_opt_bow.x_train_sel, y_train, rm_chi_opt_bow.x_test_sel, y_test, model, scoring='accuracy', print_model=False, style='ggplot')\n",
    "    plt.title(model_name)\n",
    "         \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Models with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(criterion='entropy', max_depth=4, n_estimators = 250, random_state = 1)\n",
    "clf2 = LinearSVC(C=1000, dual=False, loss='squared_hinge', max_iter=2000, penalty='l1')\n",
    "clf3 = MultinomialNB(alpha=0.1, fit_prior=True)\n",
    "clf4 = LogisticRegression(C=3, dual=False, multi_class='ovr', penalty='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8)\n",
    "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8)\n",
    "bagging3 = BaggingClassifier(base_estimator=clf3, n_estimators=10, max_samples=0.8)\n",
    "bagging4 = BaggingClassifier(base_estimator=clf4, n_estimators=10, max_samples=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves for Bagged Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_learning_curves\n",
    "\n",
    "models = [\n",
    "    bagging1, bagging2, bagging3, bagging4\n",
    "]\n",
    "labels = ['Bagging RF', 'Bagging SVC', 'Bagging NB','Bagging LR']\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for model, label, grd in zip(models, labels, grid):    \n",
    "    model_name = model.__class__.__name__\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig2 = plot_learning_curves(rm_chi_opt_bow.x_train_sel, y_train, rm_chi_opt_bow.x_test_sel, y_test, model, print_model=False, style='ggplot')\n",
    "    plt.title(label)\n",
    "         \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Scores Varied by Ensemble Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [clf1, clf2, clf3, clf4]\n",
    "labels = ['Bagging RF', 'Bagging SVC', 'Bagging NB','Bagging LR']\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for clf, label, grd in zip(clf_list, labels, grid):  \n",
    "    num_est = map(int, np.linspace(5,30,6))\n",
    "    bg_clf_cv_mean = []\n",
    "    bg_clf_cv_std = []\n",
    "    for n_est in num_est:    \n",
    "        bg_clf = BaggingClassifier(base_estimator=clf1, n_estimators=n_est, max_samples=0.8, max_features=0.8)\n",
    "        scores = cross_val_score(bg_clf, rm_chi_opt_bow.x_train_sel, y_train, cv=3, scoring='f1_weighted')\n",
    "        bg_clf_cv_mean.append(scores.mean())\n",
    "        bg_clf_cv_std.append(scores.std())\n",
    "\n",
    "    num_est = list(map(int, np.linspace(5,30,6)))\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]]) \n",
    "\n",
    "    (_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)\n",
    "                                                                                         \n",
    "    fig2 = plt.ylabel('F1-Score Weighted'); plt.xlabel('Ensemble Size'); plt.title(label + ' Score by Ensemble Size');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=30, max_samples=0.9)\n",
    "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.7)\n",
    "bagging3 = BaggingClassifier(base_estimator=clf3, n_estimators=20, max_samples=0.8)\n",
    "bagging4 = BaggingClassifier(base_estimator=clf4, n_estimators=15, max_samples=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "labels = ['Bagging RF', 'Bagging SVC', 'Bagging NB','Bagging LR']\n",
    "clf_list = [bagging1, bagging2, bagging3, bagging4]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "grid = itertools.product([0,1],repeat=4)\n",
    "\n",
    "for clf, label, grd, model_id in zip(clf_list, labels, grid, model_ids):        \n",
    "    scores = cross_val_score(clf, rm_chi_opt_bow.x_train_sel, y_train, cv=3, scoring='f1_weighted')\n",
    "    entries.append([model_id, label, 'optimized', 'tuned', '', scores.mean()])\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(entries, columns=['Model_Id', 'Model', 'Features', 'Hyper_Param', 'Best_Params', 'F1_Score'])\n",
    "models_df = result_df.groupby(['Model_Id','Model','Features','Hyper_Param', 'Best_Params']).agg(['mean'])\n",
    "models_df.columns = models_df.columns.map('_'.join)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "boosting1 = AdaBoostClassifier(base_estimator=clf1)\n",
    "boosting2 = AdaBoostClassifier(base_estimator=clf2, algorithm='SAMME')\n",
    "boosting3 = AdaBoostClassifier(base_estimator=clf3)\n",
    "boosting4 = AdaBoostClassifier(base_estimator=clf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Scores Varied by Ensemble Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "bst_list = [boosting1, boosting2, boosting3, boosting4]\n",
    "labels = ['AdaBoost RF', 'AdaBoost SVC', 'AdaBoost NB','AdaBoost LR']\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for boosting, label, grd in zip(bst_list, labels, grid):  \n",
    "    num_est = map(int, np.linspace(1,20,5))\n",
    "    bg_clf_cv_mean = []\n",
    "    bg_clf_cv_std = []\n",
    "    for n_est in num_est:\n",
    "        boosting.set_params(n_estimators=n_est)\n",
    "        scores = cross_val_score(boosting, rm_chi_opt_bow.x_train_sel, y_train, cv=3, scoring='f1_weighted')\n",
    "        bg_clf_cv_mean.append(scores.mean())\n",
    "        bg_clf_cv_std.append(scores.std())\n",
    "        print(\"F1-Score Weighted: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    \n",
    "    ax = plt.subplot(gs[grd[0], grd[1]]) \n",
    "    num_est = list(map(int, np.linspace(1,20,5)))\n",
    "    (_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)                                                                                                                                \n",
    "    fig2 = plt.ylabel('F1-Score Weighted'); plt.xlabel('Ensemble Size'); plt.title(label + ' Score by Ensemble Size');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves for Boosted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Boosting learning curve\n",
    "fig_bst = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for boosting, label, grd in zip(bst_list, labels, grid):\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig_bst = plot_learning_curves(rm_chi_opt_bow.x_train_sel, y_train, rm_chi_opt_bow.x_test_sel, y_test, boosting, print_model=False, style='ggplot')\n",
    "    plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting1 = AdaBoostClassifier(base_estimator=clf1, n_estimators=10)\n",
    "boosting2 = AdaBoostClassifier(base_estimator=clf2, n_estimators=3, algorithm='SAMME')\n",
    "boosting3 = AdaBoostClassifier(base_estimator=clf3, n_estimators=1)\n",
    "boosting4 = AdaBoostClassifier(base_estimator=clf4, n_estimators=2)\n",
    "boost_list = [boosting1, boosting2, boosting3, boosting4]\n",
    "labels_bst = ['AdaBoost RF', 'AdaBoost SVC', 'AdaBoost NB','AdaBoost LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "labels = ['AdaBoost RF', 'AdaBoost SVC', 'AdaBoost NB','AdaBoost LR']\n",
    "bst_list = [boosting1, boosting2, boosting3, boosting4]\n",
    "\n",
    "for boosting, label, model_id in zip(bst_list, labels, model_ids):\n",
    "    \n",
    "    scores = cross_val_score(boosting, rm_chi_opt_bow.x_train_sel, y_train, cv=3, scoring='f1_weighted')\n",
    "    entries.append([model_id, label, 'optimized', 'tuned', '', scores.mean()])\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(entries, columns=['Model_Id', 'Model', 'Features', 'Hyper_Param', 'Best_Params', 'F1_Score'])\n",
    "models_df = result_df.groupby(['Model_Id','Model','Features','Hyper_Param', 'Best_Params']).agg(['mean'])\n",
    "models_df.columns = models_df.columns.map('_'.join)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=clf4)\n",
    "\n",
    "labels = ['Random Forest', 'LinearSVC', 'MultinomialNB', 'Stacking LR']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "    \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(clf_list, labels, grid):\n",
    "        \n",
    "    scores = cross_val_score(clf, rm_chi_opt_bow.x_train_sel, y_train, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    if (label == 'Stacking LR'):\n",
    "        entries.append(['Stack', label, 'optimized', 'tuned', '', scores.mean()])\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot classifier accuracy    \n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(range(4), clf_cv_mean, yerr=clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.xticks(range(4), ['RF', 'SVC', 'NB', 'StackingLR'])        \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Classifier'); plt.title('Stacking Ensemble');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Stacking learning curve\n",
    "plt.figure()\n",
    "plot_learning_curves(rm_chi_opt_bow.x_train_sel, y_train, rm_chi_opt_bow.x_test_sel, y_test, sclf, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "sclf_bst = StackingClassifier(classifiers=[boosting1, boosting2, boosting3], meta_classifier=clf4)\n",
    "\n",
    "labels = ['Boosted RF', 'Boosted SVC', 'Boosted NB', 'Stacking Boosted LR']\n",
    "#clf_list = [clf1, clf2, clf3, sclf]\n",
    "bst_list = [boosting1, boosting2, boosting3, sclf_bst]\n",
    "    \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(bst_list, labels, grid):\n",
    "        \n",
    "    scores = cross_val_score(clf, rm_chi_opt_bow.x_train_sel, y_train, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    if (label == 'Stacking Boosted LR'):\n",
    "        entries.append(['Boost_Stack', label, 'optimized', 'tuned', '', scores.mean()])\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(entries, columns=['Model_Id', 'Model', 'Features', 'Hyper_Param', 'Best_Params', 'F1_Score'])\n",
    "models_df = result_df.groupby(['Model_Id', 'Model','Features','Hyper_Param', 'Best_Params']).agg(['mean'])\n",
    "models_df.columns = models_df.columns.map('_'.join)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we may "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves gave us some bad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df.boxplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Boxplot of all f1-score for all analysis\n",
    "models_df.boxplot(column=['F1_Score_mean'], by='Model_Id', figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result Barplot\n",
    "fig, (ax1) = plt.subplots(figsize=(12, 4), ncols=1, sharex=True)\n",
    "sns.barplot(y='f1_score', x='model_name', data=models_df, palette=\"colorblind\", hue='features', ax=ax1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df.sort_values(by='F1_Score_mean', ascending=False).plot(y=\"F1_Score_mean\", kind='bar', title=\"Model Evaluation Results with F1 Score\", figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
